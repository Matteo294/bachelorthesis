\chapter{Definition of Entropy}
\label{ch:entropy}
The temperature definition via
\begin{equation*}
    \frac{1}{T} = \frac{\partial S(E)}{\partial E}
\end{equation*}
is strictly dependent on the definition of the entropy and the existence of negative temperatures is mainly a consequence of this definition. 
It was already discussed in chapter \ref{ch:temperature} why it is legitimate to define the temperature as the derivative of the entropy with respect to the energy. The purpose of this section, on the other side,
is to discuss the definition of the entropy with particular regards to the definitions given by Gibbs and Boltzmann. since they have direct consequences on the existence of negative temperatures. \\
After a brief introduction over the two perspectives, which would require an entire thesis themselves to be deeply developed, the focus of the discussion will be on the comparison between the two definitions with particular emphasis on the direct consequences on the negative temperatures.

\section{Boltzmann's framework}
The conceptual basis for the Boltzmann's approach to statistical mechanics presented in Boltzmann's paper (1877b) relies on the attempt to explain the Second Law of thermodynamics via probability calculus. \\
To introduce the idea let us consider a system of $N$ particles and let us work in a $6N$ dimensional phase space with coordinates $q_1,...,q_{3N}, p_1, ... p_{3N}$ and let us consider also the $\mu$-space associated to each particle in the system, i.e. the phase space in which each single particle moves. \\
Let us parition each $\mu$-space into $m$ disjoint rectangular cells of volume $\Delta\omega$ so that $\mu = \omega_1 \cup ... \cup \omega_m$ and each cell $\omega_i$ is charaterized by an energy value $\epsilon_i$. Once specifying the mechanical state of the system, a point $x \in \Gamma$, one can 
associate a collection of $N$ points in the $\mu$-spaces, one for each particle (they are the same for each particle). For each $x$, also called \emph{microstate} of the system, one can define a \emph{macrostate} for the system by specifying the number of particles $n_i$ included in each cell $\omega_i$ in the $\mu$-space. 
Formally $Z = (n_1, ... , n_m)$ where $n_i$ is the number of particles in the cell $\omega_i \subset \mu$. \\
By this definition it is clear that more than one microstate can describe the same macrostate of the system. For each macrostate $Z_0$ the corresponding phase space volume, that is the set of corresponding microstates, is 
\begin{equation*}
    \Gamma_{Z_0} \equiv \{x \in \Gamma : Z(x) = Z_0\}
\end{equation*}
The Boltzmann's entropy of a system in a macrostate $Z$ is then defined as
\begin{equation}
    S = k_B \ln\left( \text{Vol}(\Gamma_Z)\right)
    \label{eq:Boltzmann_entropy}
\end{equation}

\section{Gibbs' framework}
Gibbs' approach to statistical mechanics is based on the idea of a statistical ensemble. To introduce this concept
let us work again in a phase space $\Gamma$ and describe the system via $3N$ canonical coordinates, so that $\Gamma$ is a $6N$ dimensional space. A point in $\Gamma$ denotes
a precise configuration of the system and it is referred to as a \emph{representative point}. \\
A given macroscopic configuration for the system can correspond to multiple microscopic configurations of the system, that is multiple points in the $\Gamma$ space might correspond to the same macroscopic state. \\
In other words, when specifying a precise macroscopic configuration, we are not referring to one system, but rather to a collection of systems which we call an \emph{ensemble}. \\
An ensemble is conveniently described my means of a \emph{density function} $\rho(q, p ,t)$ such that $\rho(q, p, t) d^{3N}qd^{3N}p$ is the number of representative points in a phase space volume $d^{3N}qd^{3N}p$. \\
Given the value of $\rho$ at time $t=0$, the evolution of the function is completely determined by means of the Hamilton equation 
\begin{gather*}
    \frac{dp_i}{dt} = -\frac{\partial \mathcal H}{\partial q_i} \\
    \frac{dq_i}{dt} = \frac{\partial \mathcal H}{\partial p_i}
\end{gather*}
More precisely the evolution of $\rho$ is determined by the \emph{Liouville's theorem} which states that
\begin{equation}
    \frac{\partial \rho}{\partial t} + \sum_{i=1}^{3N} \left(\frac{\partial \rho}{\partial p_i}\dot p_i + \frac{\partial \rho}{\partial q_i} \dot q_i\right) = 0
\end{equation}
or 
\begin{equation*}
    \frac{\partial \rho}{\partial t} = \left\{H, \rho\right\}
\end{equation*}
ADD PROOF OF THE Liouville's theorem. \\
\vspace{10pt}
In developing his theory, Gibbs' main goal was to produce a rational fundation for thermodynamics. Hence, Gibbs's work was guided by analogies between his theory and thermodynamics. \\
In his book \cite{gibbs_2010} Gibbs derived a relation in the canonical ensemble that is
\begin{equation}
    d\left\langle H \right\rangle = \theta d\sigma - \sum_i \left\langle A \right\rangle da_i
    \label{eq:fundamentaleq_Gibbs}
\end{equation}
which is formally analogue to the fundamental equation of thermodynamics
\begin{equation*}
    dU = TdS + \sum_i F_i da_i
\end{equation*}
where $\left\langle H \right\rangle$ in equation \ref{eq:fundamentaleq_Gibbs} denotes the expectation value of the hamiltonian in the canonical ensemble.
The analogy suggests that $\theta$, the so called \emph{modulus of the ensemble}, can be identifies as the temperature of the system and  and $\sigma$, defined as
\begin{equation*}
    \sigma[p_{\theta}] = - \int p_{\theta}(x) \ln \rho_{\theta}(x) \, dx
\end{equation*}
can be identified as the entropy of the system, namely the \emph{Gibbs entropy}. \\
One important point to note is that in the Gibbs' entropy is not a function on the phase space but rather a functional on the ensemble density $\rho_{\theta}$. This implies that there is no function $\chi$ on the phase space such that 
\begin{equation*}
    \left\langle \chi \right\rangle_{\theta} = \sigma[\rho_{\theta}] \quad \forall \theta
\end{equation*}
The next step is to understand whether an equation such \ref{eq:fundamentaleq_Gibbs} can be obtained in the microcanonical ensemble. Gibbs proposed (see \cite{gibbs_2010} page 124-128, 169,171) the following definitions
\begin{gather*}
    T \quad \longleftrightarrow \quad \left(\frac{\partial \ln \Omega(E)}{\partial E}\right)^{-1} \\
    S \quad \longleftrightarrow \quad \ln \Omega(E)
\end{gather*}
where 
\begin{equation}
    \Omega(E) \equiv \int_{H(x) \leq E} \, dx
    \label{eq:Omega_E}
\end{equation}
is called \emph{integrated density of states}. 

\section{Consistent thermodynamics forbids negative temperatures}
The difference between the Gibbs entropy \footnote{the Boltzmann's constant is added for dimensional arguments}
\begin{equation}
    S_G = k_B \ln \Omega(E)
    \label{eq:gibbs_entropy_formula}
\end{equation}  
and the Boltzmann's entropy 
\begin{equation}
    S_B = k_B \ln \omega(E)
    \label{eq:Boltzmann_entropy_formula}
\end{equation}
has a direct and important consequence on the existence of negative temperatures. Indeed it is easy to see that the integrated density of states defined in equation 
\ref{eq:Omega_E}, that is the number of states whose energy is less than or equal to $E$, is a monotonically increasing function of the energy, hence the temperature is always positive. On the other side,
the density of states $\omega(E)$ that enters in the Boltzmann's entropy denotes the number of states in the range $(E, E+dE)$ and there is no reason to believe that is a monotonically increasing function of the energy. In fact we have already seen
that in the case of a non interacting two level system the Boltmann's entropy leads to negative temperatures. \\
The choice of the correct microcanonical entropy is not a straightforward process since different definitions of entropy can be used untill they are able to reproduce the thermodynamics. \\
In 1991 Berdichevsky \textit{et al.} \cite{original_entropy} pointed out some arguments in favour of the Gibbs' entropy, which have then been developed by many authors. The main arguments were proposed by 
Dunkel and Hillbert in 2014 \cite{Dunkel_Hillbert} introducing the so called \emph{thermostatistical consistency condition}. \par 
\vspace{10pt} 
Let us suppose that the system is described by some control variables $\{E, V, A_i\}$ so that $S = S(E, V, A_i)$. By differentiating $S$ with respect to the control variables 
\begin{equation*}
    \begin{aligned}
        \mathrm{d} S &=\left(\frac{\partial S}{\partial E}\right) \mathrm{d} E+\left(\frac{\partial S}{\partial V}\right) \mathrm{d} V+\sum_{i}\left(\frac{\partial S}{\partial A_{i}}\right) \mathrm{d} A_{i} \\
        & \equiv \frac{1}{T} \mathrm{~d} E+\frac{p}{T} \mathrm{~d} V+\sum_{i} \frac{a_{i}}{T} \mathrm{~d} A_{i}
        \end{aligned}
\end{equation*}
If one now considers an isoentropic process for which only energy and one control variable are allowed to change, one gets that
\begin{equation}
    T\left(\frac{\partial S}{\partial A_{\mu}}\right)_{E}=-\left(\frac{\partial E}{\partial A_{\mu}}\right)_{S}=-\left\langle\frac{\partial H}{\partial A_{\mu}}\right\rangle
    \label{eq:cond1}
\end{equation}
Now, since
\begin{equation*}
    \begin{array}{l}
        T_{\mathrm{B}}(E, V, A)=\left(\frac{\partial S_{\mathrm{B}}}{\partial E}\right)^{-1}=\frac{1}{k_{\mathrm{B}}} \frac{\omega}{\omega^{\prime}}=\frac{1}{k_{\mathrm{B}}} \frac{\Omega^{\prime}}{\Omega^{\prime \prime}} \\
        T_{\mathrm{G}}(E, V, A)=\left(\frac{\partial S_{\mathrm{G}}}{\partial E}\right)^{-1}=\frac{1}{k_{\mathrm{B}}} \frac{\Omega}{\Omega^{\prime}}=\frac{1}{k_{\mathrm{B}}} \frac{\Omega}{\omega}
        \end{array}
\end{equation*}
we expect that if one of the two definitions of the entropy satisfies the condition \ref{eq:cond1}, the other one does not. \\
The Gibbs entropy satisfies the condition
\begin{equation*}\begin{aligned}
    T_{G}\left(\frac{\partial S_{G}}{\partial A_{\mu}}\right) &=\left(\frac{\partial S_{G}}{\partial E}\right)^{-1}\left(\frac{\partial S_{G}}{\partial A_{\mu}}\right)=\frac{1}{\omega} \frac{\partial}{\partial A_{\mu}} \operatorname{Tr}[\theta(E-\mathcal{H})] \\
    &=\frac{1}{\omega} \operatorname{Tr}\left[\frac{\partial \theta(E-\mathcal{H})}{\partial A_{\mu}}\right]=-\frac{1}{\omega} \operatorname{Tr}\left[\delta(E-\mathcal{H}) \frac{\partial \mathcal{H}}{\partial A_{\mu}}\right] \\
    &=-\operatorname{Tr}\left[\frac{\delta(E-\mathcal{H})}{\omega} \frac{\partial \mathcal{H}}{\partial A_{\mu}}\right]=-\operatorname{Tr}\left[\rho \frac{\partial \mathcal{H}}{\partial A_{\mu}}\right]=-\left\langle\frac{\partial \mathcal{H}}{\partial A_{\mu}}\right\rangle
\end{aligned}\end{equation*}
so the Boltzmann's cannot